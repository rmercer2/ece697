{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "## This file builds the dissimilarity matrices from one day of data\n",
    "## Required files: 'locations.csv' and 'associations_times.csv'\n",
    "## Produces files:\n",
    "##                - Average Building Time Apart Matrix: buildings_dissimilarity_mat.csv \n",
    "##                - Building Labels: building_names.csv\n",
    "##                - Average Building Time Apart Scaled Matrix: scaled_buildings_dissimilarity_mat.csv \n",
    "##                - Subset of Average Access Point Time Apart Matrix: test_dissimilarity_mat.csv\n",
    "##                - Access Point Labels: test_ids.csv \n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import csv\n",
    "import sys\n",
    "from numpy import inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data on installation_id and names as locations_df\n",
    "# schema: installation_id, name, latitude, longitude, accuracy\n",
    "locations_df = pd.read_csv('locations.csv')\n",
    "#locations_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data generated from preprocessing notebook\n",
    "# schema: (ignore 1st col (mistake in DataPreprocessing)), ap_1, ap_2, total_time, frequency\n",
    "associations_times_df = pd.read_csv('associations_times.csv')\n",
    "#associations_times_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DISSIMILARITY MATRIX B (each building to building connection)\n",
    "using all data  \n",
    "generates files for the ds matrix and the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate mappings to make look ups when building ds matrix faster\n",
    "\n",
    "map_id_names = {} # id is key, name is value\n",
    "building_ids = {} # maps building name to index between 0 and number of unique buildings\n",
    "building_indices = {} # maps index to building name\n",
    "building_names = [] # list of all building names for reference\n",
    "prev_name = None\n",
    "cnt_buildings = 0\n",
    "\n",
    "# iterate over locations dataframe\n",
    "for i, row in locations_df.iterrows():\n",
    "    # get name and id of current row\n",
    "    curr_name = row['name'].split('-')[0]\n",
    "    curr_id = row['installation_id']\n",
    "    if prev_name == None:\n",
    "        prev_name = curr_name\n",
    "    elif prev_name != curr_name:\n",
    "        # if on to new building name store info about previous\n",
    "        building_ids[prev_name] = cnt_buildings\n",
    "        building_indices[cnt_buildings] = prev_name\n",
    "        building_names.append(prev_name)\n",
    "        prev_name = curr_name\n",
    "        cnt_buildings += 1\n",
    "    # maintain map of all access point ids to respective buildings\n",
    "    if curr_id not in map_id_names:\n",
    "        map_id_names[curr_id] = curr_name\n",
    "\n",
    "building_names.append(curr_name)\n",
    "# catch last building name to store info\n",
    "if prev_name != curr_name:\n",
    "    building_ids[prev_name] = cnt_buildings\n",
    "    building_indices[cnt_buildings] = prev_name\n",
    "    building_names.append(curr_name)\n",
    "\n",
    "# n_buildings is number of unique buildings that have sensors\n",
    "n_buildings = len(building_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init matrices for calculating ds_mat by buildings\n",
    "ds_buildings_sums = np.zeros((n_buildings,n_buildings))\n",
    "ds_buildings_freq = np.zeros((n_buildings,n_buildings))\n",
    "ds_buildings_mat = np.zeros((n_buildings,n_buildings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum times and frequencies of each connect between any sensor of one building \n",
    "# to any sensor of another building for each combo of buildings\n",
    "for i, row in associations_times_df.iterrows():\n",
    "    if row['ap_1'] in map_id_names and row['ap_2'] in map_id_names:\n",
    "        name1 = map_id_names[row['ap_1']]\n",
    "        name2 = map_id_names[row['ap_2']]\n",
    "        if name1 != name2:\n",
    "            i1 = building_ids[name1]\n",
    "            i2 = building_ids[name2]\n",
    "            if i1 < i2:\n",
    "                ds_buildings_sums[i1][i2] += row['total_time']\n",
    "                ds_buildings_freq[i1][i2] += row['frequency']\n",
    "            elif i1 > i2:\n",
    "                ds_buildings_sums[i2][i1] += row['total_time']\n",
    "                ds_buildings_freq[i2][i1] += row['frequency']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comput ds matrix\n",
    "for i in range(n_buildings):\n",
    "    for j in range(i+1,n_buildings):\n",
    "        if ds_buildings_freq[i][j] != 0:\n",
    "            ds_buildings_mat[i][j] = ds_buildings_mat[j][i] = ds_buildings_sums[i][j] / ds_buildings_freq[i][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# catch and toss overflow errors\n",
    "ds_buildings_mat[np.isnan(ds_buildings_mat)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_buildings):\n",
    "    for j in range(i+1,n_buildings):\n",
    "        if not ds_buildings_mat[i][j] >= 0:\n",
    "            print(i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save building names\n",
    "np.savetxt(\"building_names.csv\", building_names, delimiter=',', fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix to csv file\n",
    "np.savetxt('buildings_dissimilarity_mat.csv', ds_buildings_mat, delimiter=',', fmt='%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DISSIMILARITY MATRIX B_1 (each building to building connection)\n",
    "try tf-idf sort of adaptation of tf-idf becuase some are much more frequent than others\n",
    "generates: scaled_buildings_dissimilarity_mat.csv\n",
    "  \n",
    "    tf = term frequency (number of times the connection between building a, b appears)\n",
    "    idf = inverse document frequency = log(N/df)\n",
    "    N = total number of connections between all buildings\n",
    "    df = document frequency (sum of freq of union of connections that building a and building b each have)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "buildings_TF_IDF = np.zeros((n_buildings,n_buildings))\n",
    "N = sum(sum(ds_buildings_freq))\n",
    "buildings_DF = ds_buildings_freq.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  \"\"\"\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# calculate idf\n",
    "for i in range(n_buildings):\n",
    "    for j in range(i+1,n_buildings):\n",
    "        if (i != j) or (ds_buildings_freq[i][j] != 0) or ((buildings_DF[i] != 0) and (buildings_DF[j] != 0)) or ((buildings_DF[i] + buildings_DF[j] - ds_buildings_freq[i][j]) != 0):\n",
    "            buildings_TF_IDF[i][j] = ds_buildings_freq[i][j] * np.log(N/(buildings_DF[i] + buildings_DF[j] - ds_buildings_freq[i][j]))\n",
    "buildings_TF_IDF[buildings_TF_IDF == inf] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_buildings_ds_mat = np.zeros((n_buildings,n_buildings))\n",
    "for i in range(n_buildings):\n",
    "    for j in range(i+1,n_buildings):\n",
    "        if (i != j):\n",
    "            scaled_buildings_ds_mat[i][j] = scaled_buildings_ds_mat[j][i] = buildings_TF_IDF[i][j] * ds_buildings_mat[i][j]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "# catch and toss overflow errors\n",
    "scaled_buildings_ds_mat[np.isnan(scaled_buildings_ds_mat)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix to csv file\n",
    "np.savetxt('scaled_buildings_dissimilarity_mat.csv', scaled_buildings_ds_mat, delimiter=',', fmt='%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DISSIMILARITY MATRIX A (each access point to access point connection)\n",
    "generates:  test dissimilarity matrix and labels for access points\n",
    "including only aps that have more than 100 connections to other aps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate hashmaps to make look ups in building matrix faster\n",
    "# ids: installation_id as key, index as value\n",
    "# indices: index as key, installation_id as value\n",
    "ids = {}\n",
    "indices = {}\n",
    "for i, row in locations_df.iterrows():\n",
    "    ids[row['installation_id']] = i\n",
    "    indices[i] = row['installation_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initalize matrices for calculations\n",
    "n = len(locations_df)\n",
    "ds_sums = np.zeros((n,n))\n",
    "ds_freq = np.zeros((n,n))\n",
    "ds_mat = np.zeros((n,n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum times\n",
    "# sum frequencies\n",
    "for i, row in associations_times_df.iterrows():\n",
    "    id1 = row['ap_1']\n",
    "    id2 = row['ap_2']\n",
    "    if id1 in ids and id2 in ids:\n",
    "        i1 = ids[id1]\n",
    "        i2 = ids[id2]\n",
    "        if i1 < i2:\n",
    "            ds_sums[i1][i2] += row['total_time']\n",
    "            ds_freq[i1][i2] += row['frequency']\n",
    "        elif i1 > i2:\n",
    "            ds_sums[i2][i1] += row['total_time']\n",
    "            ds_freq[i2][i1] += row['frequency']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3916113\n"
     ]
    }
   ],
   "source": [
    "# checking what fraction of transistions between sensors exist in test data\n",
    "cnt = 0\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        if ds_sums[i][j] != 0:\n",
    "            cnt += 1\n",
    "print(cnt/(n*n - n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute avg time for the dissimilarity matrix\n",
    "for i in range(n-1):\n",
    "    for j in range(i+1,n):\n",
    "        if ds_freq[i][j] != 0:\n",
    "            ds_mat[i][j] = ds_sums[i][j] / ds_freq[i][j]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check seeing freq of each access point in test data\n",
    "index_freq = ds_freq.sum(axis=0)\n",
    "index_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check seeing number of connections each access point has to other access points\n",
    "connections = np.count_nonzero(ds_mat, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9009 942\n"
     ]
    }
   ],
   "source": [
    "# compute how many access points have connections above threshold = 1000\n",
    "n_connected = 0\n",
    "for i in range(n):\n",
    "    if connections[i] != 0:\n",
    "        if connections[i] > 1000:\n",
    "            n_connected += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREVIOUS methods for trimming dataset down to most common connections\n",
    "#functioning_ids = {}\n",
    "#count = 0\n",
    "#for i in range(n):\n",
    "#    if index_freq[i] != 0:\n",
    "#        functioning_ids[count] = indices[i]\n",
    "#        count += 1\n",
    "\n",
    "#accurate_test = 0\n",
    "#for i in range(count):\n",
    "#    if index_freq[i] > 1500000:\n",
    "#        accurate_test += 1\n",
    "#accurate_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init and fill test_ids with interesting test_ids\n",
    "test_ids = np.zeros((n_connected,))\n",
    "c = 0\n",
    "for i in range(n):\n",
    "    if connections[i] > 1000:\n",
    "        if i not in indices:\n",
    "            print('error')\n",
    "        test_ids[c] = indices[i]\n",
    "        c += 1\n",
    "        \n",
    "# array --> dataframe\n",
    "test_ids_df = pd.DataFrame(test_ids)\n",
    "  \n",
    "# dataframe --> csv file\n",
    "test_ids_df.to_csv(\"test_ids.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init and fill test_ds_mat with data for the access points interested in\n",
    "# builds symmetric matrix of average time between access points\n",
    "test_ds_mat = np.zeros((n_connected,n_connected))\n",
    "for i in range(n_connected-2):\n",
    "    temp_i1 = ids[test_ids[i]]\n",
    "    for j in range(i+1,n_connected-1):\n",
    "        if test_ids[j] == 0:\n",
    "            print('error')\n",
    "            break\n",
    "        temp_i2 = ids[test_ids[j]]\n",
    "        test_ds_mat[i][j] = ds_mat[temp_i1][temp_i2]\n",
    "        test_ds_mat[j][i] = ds_mat[temp_i1][temp_i2]\n",
    "        \n",
    "# matrix to csv file\n",
    "np.savetxt('test_dissimilarity_mat.csv', test_ds_mat, delimiter=',', fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(942, 942)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check size of test produced\n",
    "print(len(test_ids))\n",
    "print(np.shape(test_ds_mat))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
